{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1062313,"sourceType":"datasetVersion","datasetId":589173}],"dockerImageVersionId":29955,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os, glob, random\nfrom collections import Counter\n\nimport numpy as np\nimport pandas as pd\n\nfrom PIL import Image, ImageOps\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader","metadata":{"execution":{"iopub.status.busy":"2026-01-30T12:28:31.022356Z","iopub.execute_input":"2026-01-30T12:28:31.022614Z","iopub.status.idle":"2026-01-30T12:28:32.389765Z","shell.execute_reply.started":"2026-01-30T12:28:31.022589Z","shell.execute_reply":"2026-01-30T12:28:32.388996Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"INPUT_DIR = \"/kaggle/input/crop-and-weed-detection-data-with-bounding-boxes\"\nimg_exts = (\".jpg\",\".jpeg\",\".png\")\n\nall_imgs, all_xmls, all_txts = [], [], []\nfor root, _, files in os.walk(INPUT_DIR):\n    for f in files:\n        fp = os.path.join(root, f)\n        if f.lower().endswith(img_exts):\n            all_imgs.append(fp)\n        elif f.lower().endswith(\".xml\"):\n            all_xmls.append(fp)\n        elif f.lower().endswith(\".txt\"):\n            all_txts.append(fp)\n\nprint(\"Images:\", len(all_imgs))\nprint(\"XMLs  :\", len(all_xmls))\nprint(\"TXTs  :\", len(all_txts))\nprint(\"Example image:\", all_imgs[0] if all_imgs else None)\nprint(\"Example xml  :\", all_xmls[0] if all_xmls else None)\nprint(\"Example txt  :\", all_txts[0] if all_txts else None)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T12:28:32.391865Z","iopub.execute_input":"2026-01-30T12:28:32.392090Z","iopub.status.idle":"2026-01-30T12:28:34.130179Z","shell.execute_reply.started":"2026-01-30T12:28:32.392068Z","shell.execute_reply":"2026-01-30T12:28:34.129353Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def find_image_by_stem(stem):\n    hits = []\n    for ext in img_exts:\n        hits += glob.glob(os.path.join(INPUT_DIR, \"**\", stem + ext), recursive=True)\n    return hits[0] if hits else None\n\ndf = None\n\n# --- Try VOC XML first ---\nif len(all_xmls) > 0:\n    import xml.etree.ElementTree as ET\n    \n    def find_image_by_filename(fn):\n        hits = glob.glob(os.path.join(INPUT_DIR, \"**\", fn), recursive=True)\n        return hits[0] if hits else None\n\n    def parse_voc_xml(xml_path):\n        tree = ET.parse(xml_path)\n        root = tree.getroot()\n\n        filename = root.findtext(\"filename\")\n        size = root.find(\"size\")\n        width = int(size.findtext(\"width\"))\n        height = int(size.findtext(\"height\"))\n\n        rows = []\n        for obj in root.findall(\"object\"):\n            label = obj.findtext(\"name\").strip()\n            bnd = obj.find(\"bndbox\")\n            xmin = int(float(bnd.findtext(\"xmin\")))\n            ymin = int(float(bnd.findtext(\"ymin\")))\n            xmax = int(float(bnd.findtext(\"xmax\")))\n            ymax = int(float(bnd.findtext(\"ymax\")))\n\n            rows.append({\n                \"ann_format\": \"voc_xml\",\n                \"ann_path\": xml_path,\n                \"filename\": filename,\n                \"img_path\": find_image_by_filename(filename),\n                \"width\": width, \"height\": height,\n                \"label\": label,\n                \"xmin\": xmin, \"ymin\": ymin, \"xmax\": xmax, \"ymax\": ymax\n            })\n        return rows\n\n    rows = []\n    for x in all_xmls:\n        try:\n            rows.extend(parse_voc_xml(x))\n        except:\n            pass\n\n    df = pd.DataFrame(rows)\n    print(\"Built df from XML. Rows:\", len(df))\n\n# --- Else, try YOLO txt ---\nif (df is None or len(df) == 0) and len(all_txts) > 0:\n    def is_yolo_file(txt_path):\n        try:\n            with open(txt_path, \"r\") as f:\n                lines = [l.strip() for l in f.readlines() if l.strip()]\n            if len(lines) == 0:\n                return False\n            return len(lines[0].split()) == 5\n        except:\n            return False\n\n    yolo_txts = [t for t in all_txts if is_yolo_file(t)]\n    print(\"YOLO-like txt files:\", len(yolo_txts))\n\n    def parse_yolo_txt(txt_path):\n        stem = os.path.splitext(os.path.basename(txt_path))[0]\n        img_path = find_image_by_stem(stem)\n        if img_path is None:\n            return []\n\n        img = Image.open(img_path)\n        W, H = img.size\n\n        rows = []\n        with open(txt_path, \"r\") as f:\n            for line in f:\n                parts = line.strip().split()\n                if len(parts) != 5:\n                    continue\n                cid, xc, yc, w, h = parts\n                cid = int(float(cid))\n                xc, yc, w, h = map(float, [xc, yc, w, h])\n\n                xmin = int((xc - w/2) * W)\n                ymin = int((yc - h/2) * H)\n                xmax = int((xc + w/2) * W)\n                ymax = int((yc + h/2) * H)\n\n                rows.append({\n                    \"ann_format\": \"yolo_txt\",\n                    \"ann_path\": txt_path,\n                    \"img_path\": img_path,\n                    \"width\": W, \"height\": H,\n                    \"label\": str(cid),\n                    \"xmin\": xmin, \"ymin\": ymin, \"xmax\": xmax, \"ymax\": ymax\n                })\n        return rows\n\n    rows = []\n    for t in yolo_txts:\n        try:\n            rows.extend(parse_yolo_txt(t))\n        except:\n            pass\n\n    df = pd.DataFrame(rows)\n    print(\"Built df from YOLO TXT. Rows:\", len(df))\n\nif df is None or len(df) == 0:\n    raise ValueError(\"No usable XML/YOLO annotations found.\")\n\ndf = df.dropna(subset=[\"img_path\"]).reset_index(drop=True)\nprint(\"Final df rows:\", len(df))\nprint(\"Columns:\", df.columns.tolist())\ndf.head()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T12:28:34.134434Z","iopub.execute_input":"2026-01-30T12:28:34.134777Z","iopub.status.idle":"2026-01-30T12:29:31.348696Z","shell.execute_reply.started":"2026-01-30T12:28:34.134741Z","shell.execute_reply":"2026-01-30T12:29:31.347906Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Total boxes:\", len(df))\nprint(\"Unique label files:\", df[\"ann_path\"].nunique())\nprint(\"Unique images:\", df[\"img_path\"].nunique())\nprint(\"Unique labels:\", df[\"label\"].nunique())\nprint(\"Labels:\", df[\"label\"].unique())\n\nlabel_counts = df[\"label\"].value_counts()\ndisplay(label_counts)\n\nplt.figure()\nlabel_counts.plot(kind=\"bar\")\nplt.title(\"Class distribution (box count)\")\nplt.ylabel(\"Boxes\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T12:29:31.352102Z","iopub.execute_input":"2026-01-30T12:29:31.352323Z","iopub.status.idle":"2026-01-30T12:29:31.523074Z","shell.execute_reply.started":"2026-01-30T12:29:31.352301Z","shell.execute_reply":"2026-01-30T12:29:31.522268Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"boxes_per_image = df.groupby(\"img_path\").size()\ndisplay(boxes_per_image.describe())\n\nplt.figure()\nboxes_per_image.plot(kind=\"hist\", bins=30)\nplt.title(\"Boxes per image\")\nplt.xlabel(\"#boxes\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T12:29:31.525198Z","iopub.execute_input":"2026-01-30T12:29:31.525480Z","iopub.status.idle":"2026-01-30T12:29:31.683979Z","shell.execute_reply.started":"2026-01-30T12:29:31.525452Z","shell.execute_reply":"2026-01-30T12:29:31.683292Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df[\"box_w\"] = (df[\"xmax\"] - df[\"xmin\"]).clip(lower=1)\ndf[\"box_h\"] = (df[\"ymax\"] - df[\"ymin\"]).clip(lower=1)\ndf[\"box_area\"] = df[\"box_w\"] * df[\"box_h\"]\ndf[\"box_ar\"] = df[\"box_w\"] / df[\"box_h\"]\n\ndisplay(df[[\"box_w\",\"box_h\",\"box_area\",\"box_ar\"]].describe())\n\nplt.figure()\ndf[\"box_area\"].plot(kind=\"hist\", bins=50)\nplt.title(\"Bounding box area distribution\")\nplt.xlabel(\"Area (px^2)\")\nplt.show()\n\nplt.figure()\ndf[\"box_ar\"].plot(kind=\"hist\", bins=50)\nplt.title(\"Bounding box aspect ratio distribution\")\nplt.xlabel(\"w/h\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T12:29:31.685225Z","iopub.execute_input":"2026-01-30T12:29:31.685467Z","iopub.status.idle":"2026-01-30T12:29:32.071256Z","shell.execute_reply.started":"2026-01-30T12:29:31.685443Z","shell.execute_reply":"2026-01-30T12:29:32.070604Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"invalid = df[(df.xmin >= df.xmax) | (df.ymin >= df.ymax)]\nprint(\"Invalid boxes:\", len(invalid))\n\noob = df[(df.xmin < 0) | (df.ymin < 0) | (df.xmax > df.width) | (df.ymax > df.height)]\nprint(\"Out-of-bounds boxes:\", len(oob))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T12:29:32.072155Z","iopub.execute_input":"2026-01-30T12:29:32.072396Z","iopub.status.idle":"2026-01-30T12:29:32.080886Z","shell.execute_reply.started":"2026-01-30T12:29:32.072372Z","shell.execute_reply":"2026-01-30T12:29:32.080223Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import PIL.ImageDraw as ImageDraw\n\ndef draw_boxes(img, subdf):\n    img = img.copy()\n    d = ImageDraw.Draw(img)\n    for _, r in subdf.iterrows():\n        d.rectangle([r.xmin, r.ymin, r.xmax, r.ymax], width=3)\n        d.text((r.xmin, max(0, r.ymin-12)), str(r.label))\n    return img\n\nsample_imgs = random.sample(list(df[\"img_path\"].unique()), k=min(9, df[\"img_path\"].nunique()))\nplt.figure(figsize=(12,12))\nfor i, p in enumerate(sample_imgs, 1):\n    img = Image.open(p).convert(\"RGB\")\n    sub = df[df[\"img_path\"] == p]\n    plt.subplot(3,3,i)\n    plt.imshow(draw_boxes(img, sub))\n    plt.axis(\"off\")\n    plt.title(os.path.basename(p))\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T12:29:32.082014Z","iopub.execute_input":"2026-01-30T12:29:32.082280Z","iopub.status.idle":"2026-01-30T12:29:33.066564Z","shell.execute_reply.started":"2026-01-30T12:29:32.082254Z","shell.execute_reply":"2026-01-30T12:29:33.065774Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sample_paths = random.sample(all_imgs, k=min(300, len(all_imgs)))\nmeans, stds = [], []\n\nfor p in sample_paths:\n    im = Image.open(p).convert(\"RGB\")\n    arr = np.asarray(im).astype(np.float32) / 255.0\n    means.append(arr.mean(axis=(0,1)))\n    stds.append(arr.std(axis=(0,1)))\n\nmean_rgb = np.mean(means, axis=0)\nstd_rgb  = np.mean(stds, axis=0)\n\nprint(\"Mean RGB:\", mean_rgb)\nprint(\"Std  RGB:\", std_rgb)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T12:29:33.067875Z","iopub.execute_input":"2026-01-30T12:29:33.068119Z","iopub.status.idle":"2026-01-30T12:29:40.208448Z","shell.execute_reply.started":"2026-01-30T12:29:33.068094Z","shell.execute_reply":"2026-01-30T12:29:40.207634Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df[\"label_id\"] = df[\"label\"].astype(int)\nnum_classes = df[\"label_id\"].nunique()\nprint(\"num_classes:\", num_classes)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T12:29:40.209520Z","iopub.execute_input":"2026-01-30T12:29:40.209749Z","iopub.status.idle":"2026-01-30T12:29:40.216315Z","shell.execute_reply.started":"2026-01-30T12:29:40.209727Z","shell.execute_reply":"2026-01-30T12:29:40.215576Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"unique_imgs = df[\"img_path\"].unique().tolist()\nrandom.shuffle(unique_imgs)\n\nsplit = int(0.8 * len(unique_imgs))\ntrain_imgs = set(unique_imgs[:split])\nval_imgs   = set(unique_imgs[split:])\n\ntrain_df = df[df[\"img_path\"].isin(train_imgs)].reset_index(drop=True)\nval_df   = df[df[\"img_path\"].isin(val_imgs)].reset_index(drop=True)\n\nprint(\"Train boxes:\", len(train_df), \"| Val boxes:\", len(val_df))\nprint(\"Train images:\", len(train_imgs), \"| Val images:\", len(val_imgs))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T12:29:40.217070Z","iopub.execute_input":"2026-01-30T12:29:40.217350Z","iopub.status.idle":"2026-01-30T12:29:40.305039Z","shell.execute_reply.started":"2026-01-30T12:29:40.217313Z","shell.execute_reply":"2026-01-30T12:29:40.304344Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class CropDataset(Dataset):\n    def __init__(self, ann_df, out_size=224, augment=False, mean=None, std=None):\n        self.df = ann_df\n        self.out_size = out_size\n        self.augment = augment\n\n        self.mean = torch.tensor(mean, dtype=torch.float32).view(3,1,1) if mean is not None else None\n        self.std  = torch.tensor(std, dtype=torch.float32).view(3,1,1) if std is not None else None\n\n    def __len__(self):\n        return len(self.df)\n\n    def _to_tensor(self, img):\n        arr = np.asarray(img).astype(np.float32) / 255.0\n        arr = np.transpose(arr, (2,0,1))\n        x = torch.tensor(arr, dtype=torch.float32)\n        if self.mean is not None and self.std is not None:\n            x = (x - self.mean) / (self.std + 1e-6)\n        return x\n\n    def __getitem__(self, idx):\n        r = self.df.iloc[idx]\n        img = Image.open(r.img_path).convert(\"RGB\")\n\n        # clip bbox to image bounds\n        xmin = max(0, int(r.xmin)); ymin = max(0, int(r.ymin))\n        xmax = min(int(r.width), int(r.xmax)); ymax = min(int(r.height), int(r.ymax))\n        if xmax <= xmin: xmax = xmin + 1\n        if ymax <= ymin: ymax = ymin + 1\n\n        crop = img.crop((xmin, ymin, xmax, ymax)).resize((self.out_size, self.out_size))\n\n        # simple augmentations\n        if self.augment:\n            if random.random() < 0.5:\n                crop = crop.transpose(Image.FLIP_LEFT_RIGHT)\n            if random.random() < 0.1:\n                crop = crop.transpose(Image.FLIP_TOP_BOTTOM)\n\n        x = self._to_tensor(crop)\n        y = int(r.label_id)\n        return x, y\n\nmean_list = mean_rgb.tolist()\nstd_list  = std_rgb.tolist()\n\ntrain_ds = CropDataset(train_df, out_size=224, augment=True, mean=mean_list, std=std_list)\nval_ds   = CropDataset(val_df, out_size=224, augment=False, mean=mean_list, std=std_list)\n\ntrain_loader = DataLoader(train_ds, batch_size=32, shuffle=True, num_workers=2, pin_memory=True)\nval_loader   = DataLoader(val_ds, batch_size=32, shuffle=False, num_workers=2, pin_memory=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T12:29:40.306072Z","iopub.execute_input":"2026-01-30T12:29:40.306280Z","iopub.status.idle":"2026-01-30T12:29:40.326044Z","shell.execute_reply.started":"2026-01-30T12:29:40.306259Z","shell.execute_reply":"2026-01-30T12:29:40.325268Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class MLP(nn.Module):\n    def __init__(self, dim, hidden_dim, dropout=0.0):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(dim, hidden_dim),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(hidden_dim, dim),\n            nn.Dropout(dropout),\n        )\n    def forward(self, x):\n        return self.net(x)\n\nclass TransformerEncoderBlock(nn.Module):\n    def __init__(self, dim, heads, mlp_ratio=4.0, dropout=0.1):\n        super().__init__()\n        self.norm1 = nn.LayerNorm(dim)\n        self.attn = nn.MultiheadAttention(embed_dim=dim, num_heads=heads, dropout=dropout, batch_first=True)\n        self.norm2 = nn.LayerNorm(dim)\n        self.mlp = MLP(dim, int(dim*mlp_ratio), dropout=dropout)\n\n    def forward(self, x):\n        x_norm = self.norm1(x)\n        attn_out, _ = self.attn(x_norm, x_norm, x_norm, need_weights=False)\n        x = x + attn_out\n        x = x + self.mlp(self.norm2(x))\n        return x\n\nclass ViT(nn.Module):\n    def __init__(self, img_size=224, patch_size=16, in_chans=3, num_classes=2,\n                 dim=256, depth=6, heads=8, mlp_ratio=4.0, dropout=0.1):\n        super().__init__()\n        assert img_size % patch_size == 0\n\n        self.num_patches = (img_size // patch_size) ** 2\n        self.patch_embed = nn.Conv2d(in_chans, dim, kernel_size=patch_size, stride=patch_size)\n\n        self.cls_token = nn.Parameter(torch.zeros(1, 1, dim))\n        self.pos_embed = nn.Parameter(torch.zeros(1, 1 + self.num_patches, dim))\n        self.pos_drop = nn.Dropout(dropout)\n\n        self.blocks = nn.ModuleList([\n            TransformerEncoderBlock(dim, heads, mlp_ratio=mlp_ratio, dropout=dropout)\n            for _ in range(depth)\n        ])\n        self.norm = nn.LayerNorm(dim)\n        self.head = nn.Linear(dim, num_classes)\n\n        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n        nn.init.trunc_normal_(self.cls_token, std=0.02)\n        nn.init.trunc_normal_(self.head.weight, std=0.02)\n        nn.init.zeros_(self.head.bias)\n\n    def forward(self, x):\n        B = x.size(0)\n        x = self.patch_embed(x)                 # (B, D, H/ps, W/ps)\n        x = x.flatten(2).transpose(1, 2)        # (B, N, D)\n\n        cls = self.cls_token.expand(B, -1, -1)  # (B,1,D)\n        x = torch.cat([cls, x], dim=1)          # (B,1+N,D)\n        x = x + self.pos_embed\n        x = self.pos_drop(x)\n\n        for blk in self.blocks:\n            x = blk(x)\n\n        x = self.norm(x)\n        return self.head(x[:, 0])               # CLS token\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T12:29:40.327168Z","iopub.execute_input":"2026-01-30T12:29:40.327495Z","iopub.status.idle":"2026-01-30T12:29:40.345162Z","shell.execute_reply.started":"2026-01-30T12:29:40.327461Z","shell.execute_reply":"2026-01-30T12:29:40.344488Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\n\nclass MLP(nn.Module):\n    def __init__(self, dim, hidden_dim, dropout=0.0):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(dim, hidden_dim),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(hidden_dim, dim),\n            nn.Dropout(dropout),\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\nclass TransformerEncoderBlock(nn.Module):\n    def __init__(self, dim, heads, mlp_ratio=4.0, dropout=0.1):\n        super().__init__()\n        self.norm1 = nn.LayerNorm(dim)\n        self.attn = nn.MultiheadAttention(\n            embed_dim=dim,\n            num_heads=heads,\n            dropout=dropout\n        )\n        self.norm2 = nn.LayerNorm(dim)\n        self.mlp = MLP(dim, int(dim * mlp_ratio), dropout=dropout)\n\n    def forward(self, x):\n        # x: (B, N, D)\n        x_norm = self.norm1(x)\n\n        # MultiheadAttention expects (N, B, D)\n        x_attn = x_norm.transpose(0, 1)\n        attn_out, _ = self.attn(x_attn, x_attn, x_attn)\n        attn_out = attn_out.transpose(0, 1)\n\n        x = x + attn_out\n        x = x + self.mlp(self.norm2(x))\n        return x\n\n\ndef trunc_normal_(tensor, mean=0.0, std=1.0, a=-2.0, b=2.0):\n    with torch.no_grad():\n        size = tensor.shape\n        tmp = tensor.new_empty(size + (4,)).normal_()\n        valid = (tmp > a) & (tmp < b)\n        ind = valid.max(-1, keepdim=True)[1]\n        tensor.copy_(tmp.gather(-1, ind).squeeze(-1))\n        tensor.mul_(std).add_(mean)\n        return tensor\n\n\nclass ViT(nn.Module):\n    def __init__(\n        self,\n        img_size=224,\n        patch_size=16,\n        in_chans=3,\n        num_classes=2,\n        dim=256,\n        depth=6,\n        heads=8,\n        mlp_ratio=4.0,\n        dropout=0.1,\n    ):\n        super().__init__()\n        assert img_size % patch_size == 0\n\n        self.num_patches = (img_size // patch_size) ** 2\n\n        # Patch embedding\n        self.patch_embed = nn.Conv2d(\n            in_chans,\n            dim,\n            kernel_size=patch_size,\n            stride=patch_size,\n        )\n\n        # Tokens\n        self.cls_token = nn.Parameter(torch.zeros(1, 1, dim))\n        self.pos_embed = nn.Parameter(\n            torch.zeros(1, 1 + self.num_patches, dim)\n        )\n        self.pos_drop = nn.Dropout(dropout)\n\n        # Transformer blocks\n        self.blocks = nn.ModuleList([\n            TransformerEncoderBlock(dim, heads, mlp_ratio, dropout)\n            for _ in range(depth)\n        ])\n\n        self.norm = nn.LayerNorm(dim)\n        self.head = nn.Linear(dim, num_classes)\n\n        # Initialization\n        trunc_normal_(self.pos_embed, std=0.02)\n        trunc_normal_(self.cls_token, std=0.02)\n        trunc_normal_(self.head.weight, std=0.02)\n        nn.init.zeros_(self.head.bias)\n\n    def forward(self, x):\n        B = x.size(0)\n\n        x = self.patch_embed(x)                  # (B, D, H/ps, W/ps)\n        x = x.flatten(2).transpose(1, 2)         # (B, N, D)\n\n        cls = self.cls_token.expand(B, -1, -1)   # (B, 1, D)\n        x = torch.cat([cls, x], dim=1)           # (B, 1+N, D)\n\n        x = x + self.pos_embed\n        x = self.pos_drop(x)\n\n        for blk in self.blocks:\n            x = blk(x)\n\n        x = self.norm(x)\n        return self.head(x[:, 0])                # CLS token","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T12:29:40.346155Z","iopub.execute_input":"2026-01-30T12:29:40.346394Z","iopub.status.idle":"2026-01-30T12:29:40.368485Z","shell.execute_reply.started":"2026-01-30T12:29:40.346364Z","shell.execute_reply":"2026-01-30T12:29:40.367838Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = ViT(\n    img_size=224,\n    patch_size=16,\n    num_classes=2,\n    dim=256,\n    depth=6,\n    heads=8,\n    dropout=0.1\n).to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T12:29:40.369771Z","iopub.execute_input":"2026-01-30T12:29:40.370093Z","iopub.status.idle":"2026-01-30T12:29:43.845579Z","shell.execute_reply.started":"2026-01-30T12:29:40.370060Z","shell.execute_reply":"2026-01-30T12:29:43.844900Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Class weights (helps because your classes are imbalanced: 0=1212, 1=860 approx)\ncounts = train_df[\"label_id\"].value_counts().sort_index()\nw0 = 1.0 / counts.iloc[0]\nw1 = 1.0 / counts.iloc[1]\nclass_weights = torch.tensor([w0, w1], dtype=torch.float32).to(device)\n\ncriterion = nn.CrossEntropyLoss(weight=class_weights)\noptimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=0.05)\n\nprint(\"Class counts:\", counts.to_dict())\nprint(\"Class weights:\", class_weights.detach().cpu().numpy())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T12:29:43.846581Z","iopub.execute_input":"2026-01-30T12:29:43.846794Z","iopub.status.idle":"2026-01-30T12:29:43.858422Z","shell.execute_reply.started":"2026-01-30T12:29:43.846773Z","shell.execute_reply":"2026-01-30T12:29:43.857384Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def accuracy_from_logits(logits, y):\n    preds = torch.argmax(logits, dim=1)\n    return (preds == y).float().mean().item()\n\ndef run_epoch(loader, train=True):\n    model.train() if train else model.eval()\n\n    total_loss = 0.0\n    total_acc = 0.0\n    n = 0\n\n    for x, y in loader:\n        x = x.to(device, non_blocking=True)\n        y = y.to(device, non_blocking=True)\n\n        if train:\n            optimizer.zero_grad()\n\n        with torch.set_grad_enabled(train):\n            logits = model(x)\n            loss = criterion(logits, y)\n\n            if train:\n                loss.backward()\n                optimizer.step()\n\n        bs = x.size(0)\n        total_loss += loss.item() * bs\n        total_acc  += accuracy_from_logits(logits, y) * bs\n        n += bs\n\n    return total_loss / n, total_acc / n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T12:29:43.859541Z","iopub.execute_input":"2026-01-30T12:29:43.859773Z","iopub.status.idle":"2026-01-30T12:29:43.872656Z","shell.execute_reply.started":"2026-01-30T12:29:43.859750Z","shell.execute_reply":"2026-01-30T12:29:43.871835Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"epochs = 15\nhistory = {\"train_loss\":[], \"train_acc\":[], \"val_loss\":[], \"val_acc\":[]}\n\nfor e in range(1, epochs+1):\n    tr_loss, tr_acc = run_epoch(train_loader, train=True)\n    va_loss, va_acc = run_epoch(val_loader, train=False)\n\n    history[\"train_loss\"].append(tr_loss)\n    history[\"train_acc\"].append(tr_acc)\n    history[\"val_loss\"].append(va_loss)\n    history[\"val_acc\"].append(va_acc)\n\n    print(f\"Epoch {e:02d} | train loss {tr_loss:.4f} acc {tr_acc:.4f} | val loss {va_loss:.4f} acc {va_acc:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T12:29:43.873565Z","iopub.execute_input":"2026-01-30T12:29:43.873782Z","iopub.status.idle":"2026-01-30T12:33:09.109880Z","shell.execute_reply.started":"2026-01-30T12:29:43.873761Z","shell.execute_reply":"2026-01-30T12:33:09.108934Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure()\nplt.plot(history[\"train_loss\"], label=\"train_loss\")\nplt.plot(history[\"val_loss\"], label=\"val_loss\")\nplt.legend()\nplt.title(\"Loss\")\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T12:33:09.111150Z","iopub.execute_input":"2026-01-30T12:33:09.111395Z","iopub.status.idle":"2026-01-30T12:33:09.378939Z","shell.execute_reply.started":"2026-01-30T12:33:09.111369Z","shell.execute_reply":"2026-01-30T12:33:09.378133Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure()\nplt.plot(history[\"train_acc\"], label=\"train_acc\")\nplt.plot(history[\"val_acc\"], label=\"val_acc\")\nplt.legend()\nplt.title(\"Accuracy\")\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_preds(loader):\n    model.eval()\n    ys, ps = [], []\n    with torch.no_grad():\n        for x, y in loader:\n            x = x.to(device)\n            logits = model(x)\n            pred = torch.argmax(logits, dim=1).cpu().numpy()\n            ps.extend(pred.tolist())\n            ys.extend(y.numpy().tolist())\n    return np.array(ys), np.array(ps)\n\ny_true, y_pred = get_preds(val_loader)\n\ncm = np.zeros((2,2), dtype=int)\nfor t,p in zip(y_true, y_pred):\n    cm[t,p] += 1\n\nprint(\"Confusion Matrix (rows=true, cols=pred):\")\nprint(cm)\n\nplt.figure(figsize=(5,5))\nplt.imshow(cm)\nplt.title(\"Confusion Matrix\")\nplt.xlabel(\"Pred\")\nplt.ylabel(\"True\")\nplt.colorbar()\nplt.xticks([0,1], [\"0\",\"1\"])\nplt.yticks([0,1], [\"0\",\"1\"])\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T12:33:09.380504Z","iopub.execute_input":"2026-01-30T12:33:09.380886Z","iopub.status.idle":"2026-01-30T12:33:12.469360Z","shell.execute_reply.started":"2026-01-30T12:33:09.380847Z","shell.execute_reply":"2026-01-30T12:33:12.468626Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"torch.save({\n    \"model_state\": model.state_dict(),\n    \"label_map\": {\"0\":0, \"1\":1},\n    \"mean_rgb\": mean_list,\n    \"std_rgb\": std_list\n}, \"vit_from_scratch_cropweed.pth\")\n\nprint(\"Saved: vit_from_scratch_cropweed.pth\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T12:33:12.470665Z","iopub.execute_input":"2026-01-30T12:33:12.470925Z","iopub.status.idle":"2026-01-30T12:33:12.501253Z","shell.execute_reply.started":"2026-01-30T12:33:12.470899Z","shell.execute_reply":"2026-01-30T12:33:12.500587Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport numpy as np\nfrom PIL import Image\n\n# --------- load checkpoint ----------\nCKPT_PATH = \"vit_from_scratch_cropweed.pth\"  # adjust if different\n\nckpt = torch.load(CKPT_PATH, map_location=\"cpu\")\n\n# mean/std from training (saved in ckpt)\nmean_list = ckpt[\"mean_rgb\"]\nstd_list  = ckpt[\"std_rgb\"]\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# --------- rebuild your ViT (same class as training) ----------\n# Ensure these are already defined in your notebook:\n# trunc_normal_, MLP, TransformerEncoderBlock, ViT\n\nmodel = ViT(img_size=224, patch_size=16, num_classes=2, dim=256, depth=6, heads=8, dropout=0.1).to(device)\nmodel.load_state_dict(ckpt[\"model_state\"], strict=True)\nmodel.eval()\n\nid2name = {0: \"Class 0\", 1: \"Class 1\"}  # rename later if you know which is crop/weed\n\nmean_t = torch.tensor(mean_list, dtype=torch.float32).view(3,1,1)\nstd_t  = torch.tensor(std_list, dtype=torch.float32).view(3,1,1)\n\ndef preprocess_pil(pil_img):\n    pil_img = pil_img.convert(\"RGB\").resize((224,224))\n    arr = np.asarray(pil_img).astype(np.float32) / 255.0\n    arr = np.transpose(arr, (2,0,1))  # CHW\n    x = torch.tensor(arr, dtype=torch.float32)\n    x = (x - mean_t) / (std_t + 1e-6)\n    return x.unsqueeze(0)  # (1,3,224,224)\n\n@torch.no_grad()\ndef predict_pil(pil_img):\n    x = preprocess_pil(pil_img).to(device)\n    logits = model(x)\n    probs = torch.softmax(logits, dim=1).cpu().numpy()[0]\n    pred_id = int(np.argmax(probs))\n    return pred_id, probs\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T12:33:12.502599Z","iopub.execute_input":"2026-01-30T12:33:12.502947Z","iopub.status.idle":"2026-01-30T12:33:12.600428Z","shell.execute_reply.started":"2026-01-30T12:33:12.502911Z","shell.execute_reply":"2026-01-30T12:33:12.599687Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"query_path = \"/kaggle/input/crop-and-weed-detection-data-with-bounding-boxes/agri_data/data/agri_0_1173.jpeg\"  # change this\nimg = Image.open(query_path)\n\npred_id, probs = predict_pil(img)\nprint(\"Prediction:\", id2name[pred_id])\nprint(\"Probabilities:\", probs)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T12:33:12.601681Z","iopub.execute_input":"2026-01-30T12:33:12.602023Z","iopub.status.idle":"2026-01-30T12:33:12.627181Z","shell.execute_reply.started":"2026-01-30T12:33:12.601989Z","shell.execute_reply":"2026-01-30T12:33:12.626396Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def crop_with_box(pil_img, xmin, ymin, xmax, ymax):\n    W, H = pil_img.size\n    xmin = max(0, min(int(xmin), W-1))\n    ymin = max(0, min(int(ymin), H-1))\n    xmax = max(xmin+1, min(int(xmax), W))\n    ymax = max(ymin+1, min(int(ymax), H))\n    return pil_img.crop((xmin, ymin, xmax, ymax))\n\n# example box (edit)\nxmin, ymin, xmax, ymax = 50, 50, 250, 250\ncrop = crop_with_box(img, xmin, ymin, xmax, ymax)\n\npred_id, probs = predict_pil(crop)\nprint(\"Crop prediction:\", id2name[pred_id])\nprint(\"Probabilities:\", probs)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T12:33:12.628598Z","iopub.execute_input":"2026-01-30T12:33:12.628952Z","iopub.status.idle":"2026-01-30T12:33:12.645634Z","shell.execute_reply.started":"2026-01-30T12:33:12.628915Z","shell.execute_reply":"2026-01-30T12:33:12.644795Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from PIL import Image, ImageDraw, ImageFont\n\n# change these names if you know the real class meaning\nid2name = {0: \"Class 0\", 1: \"Class 1\"}  # e.g. {0:\"Crop\",1:\"Weed\"} if confirmed\n\ndef draw_label_on_image(img, text, xy=(10,10)):\n    \"\"\"\n    Draws a readable label (with background) on the image using Pillow.\n    \"\"\"\n    img = img.copy()\n    draw = ImageDraw.Draw(img)\n\n    # Try a default font (works on Kaggle); fallback to built-in\n    try:\n        font = ImageFont.truetype(\"/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf\", 22)\n    except:\n        font = ImageFont.load_default()\n\n    # Measure text box\n    # (textbbox exists in newer Pillow; fallback if needed)\n    try:\n        bbox = draw.textbbox(xy, text, font=font)\n        tw = bbox[2] - bbox[0]\n        th = bbox[3] - bbox[1]\n    except:\n        tw, th = draw.textsize(text, font=font)\n\n    pad = 6\n    x, y = xy\n    rect = [x - pad, y - pad, x + tw + pad, y + th + pad]\n    draw.rectangle(rect, fill=(0, 0, 0))   # black background\n    draw.text((x, y), text, fill=(255, 255, 255), font=font)  # white text\n    return img\n\ndef draw_bbox(img, xmin, ymin, xmax, ymax, color=(255, 0, 0), width=3):\n    img = img.copy()\n    draw = ImageDraw.Draw(img)\n    draw.rectangle([xmin, ymin, xmax, ymax], outline=color, width=width)\n    return img","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T12:33:12.646961Z","iopub.execute_input":"2026-01-30T12:33:12.647315Z","iopub.status.idle":"2026-01-30T12:33:12.658131Z","shell.execute_reply.started":"2026-01-30T12:33:12.647279Z","shell.execute_reply":"2026-01-30T12:33:12.657172Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom PIL import Image\n\ndef predict_and_show_full(img_path):\n    img = Image.open(img_path).convert(\"RGB\")\n\n    pred_id, probs = predict_pil(img)\n    conf = float(probs[pred_id])\n    text = f\"{id2name[pred_id]} | conf={conf:.3f} | probs={np.round(probs, 3)}\"\n\n    vis = draw_label_on_image(img, text, xy=(10,10))\n\n    plt.figure(figsize=(7,7))\n    plt.imshow(vis)\n    plt.axis(\"off\")\n    plt.show()\n\n    return pred_id, probs\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T12:33:12.659559Z","iopub.execute_input":"2026-01-30T12:33:12.659914Z","iopub.status.idle":"2026-01-30T12:33:12.667901Z","shell.execute_reply.started":"2026-01-30T12:33:12.659878Z","shell.execute_reply":"2026-01-30T12:33:12.667186Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pred_id, probs = predict_and_show_full(\"/kaggle/input/crop-and-weed-detection-data-with-bounding-boxes/agri_data/data/agri_0_1173.jpeg\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T12:33:12.669024Z","iopub.execute_input":"2026-01-30T12:33:12.669350Z","iopub.status.idle":"2026-01-30T12:33:12.900827Z","shell.execute_reply.started":"2026-01-30T12:33:12.669313Z","shell.execute_reply":"2026-01-30T12:33:12.900138Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def predict_and_show_crop(img_path, xmin, ymin, xmax, ymax, show_bbox=True):\n    img = Image.open(img_path).convert(\"RGB\")\n\n    # clip bbox to image bounds\n    W, H = img.size\n    xmin = max(0, min(int(xmin), W-1))\n    ymin = max(0, min(int(ymin), H-1))\n    xmax = max(xmin+1, min(int(xmax), W))\n    ymax = max(ymin+1, min(int(ymax), H))\n\n    crop = img.crop((xmin, ymin, xmax, ymax))\n\n    pred_id, probs = predict_pil(crop)\n    conf = float(probs[pred_id])\n    text = f\"{id2name[pred_id]} | conf={conf:.3f} | probs={np.round(probs, 3)}\"\n\n    # show original image with bbox + label\n    if show_bbox:\n        img_box = draw_bbox(img, xmin, ymin, xmax, ymax)\n        img_box = draw_label_on_image(img_box, text, xy=(10,10))\n        plt.figure(figsize=(7,7))\n        plt.imshow(img_box)\n        plt.axis(\"off\")\n        plt.show()\n\n    # show the crop too (what the model actually saw)\n    crop_vis = draw_label_on_image(crop.resize((224,224)), text, xy=(10,10))\n    plt.figure(figsize=(5,5))\n    plt.imshow(crop_vis)\n    plt.axis(\"off\")\n    plt.title(\"Cropped region (model input)\")\n    plt.show()\n\n    return pred_id, probs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T12:33:12.901881Z","iopub.execute_input":"2026-01-30T12:33:12.902128Z","iopub.status.idle":"2026-01-30T12:33:12.911243Z","shell.execute_reply.started":"2026-01-30T12:33:12.902106Z","shell.execute_reply":"2026-01-30T12:33:12.910510Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pred_id, probs = predict_and_show_crop(\n    \"/kaggle/input/crop-and-weed-detection-data-with-bounding-boxes/agri_data/data/agri_0_1173.jpeg\",\n    xmin=50, ymin=50, xmax=250, ymax=250\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T12:33:12.912166Z","iopub.execute_input":"2026-01-30T12:33:12.912378Z","iopub.status.idle":"2026-01-30T12:33:13.175693Z","shell.execute_reply.started":"2026-01-30T12:33:12.912358Z","shell.execute_reply":"2026-01-30T12:33:13.174959Z"}},"outputs":[],"execution_count":null}]}